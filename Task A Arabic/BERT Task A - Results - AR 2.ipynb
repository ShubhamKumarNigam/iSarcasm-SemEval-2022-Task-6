{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. [Hugginface](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix)\n",
    "2. [Sentiment Analysis with BERT](https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=PGnlRWvkY-2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['not_sarcastic', 'sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./\"\n",
    "\n",
    "TypeI=\"\"\n",
    "TypeII=\"TweetPreprocessed.\"\n",
    "TypeIII=\"HalfPreprocessed.\"\n",
    "TypeIV=\"FullyPreprocessed.\"\n",
    "\n",
    "pathO0=\"TaskA.Ar.train.NotAugmented.\"\n",
    "pathO1=\"TaskA.Ar.train.Augmented.\"\n",
    "pathO2=\"TaskA.Ar.train.Augmented.NotEmbedding.\"\n",
    "\n",
    "pathE0=\"TaskA.Ar.train.Augmented.NotBalanced.\"\n",
    "pathE1=\"TaskA.Ar.train.Augmented.Balanced.Original.\"\n",
    "pathE2=\"TaskA.Ar.train.Augmented.Balanced.NotOriginal.\"\n",
    "pathE3=\"TaskA.Ar.train.Augmented.Balanced.NotOriginal.NotEmbedding.\"\n",
    "\n",
    "\n",
    "pathB0=\"TaskA.Ar.train.Augmented.Biased0.Original.\"\n",
    "pathB1=\"TaskA.Ar.train.Augmented.Biased1.Original.\"\n",
    "pathB2=\"TaskA.Ar.train.Augmented.Biased2.Original.\"\n",
    "pathB3=\"TaskA.Ar.train.Augmented.Biased3.Original.\"\n",
    "pathB4=\"TaskA.Ar.train.Augmented.Biased4.Original.\"\n",
    "pathB5=\"TaskA.Ar.train.Augmented.Biased5.Original.\"\n",
    "pathB6=\"TaskA.Ar.train.Augmented.Biased6.Original.\"\n",
    "pathB7=\"TaskA.Ar.train.Augmented.Biased7.Original.\"\n",
    "pathB8=\"TaskA.Ar.train.Augmented.Biased8.Original.\"\n",
    "pathB9=\"TaskA.Ar.train.Augmented.Biased9.Original.\"\n",
    "\n",
    "pathVal=\"TaskA.Ar.Basic.Val.\"\n",
    "pathTest=\"TaskA.Ar.Basic.Test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskA.Ar.train.Augmented.Biased2.Original.\n",
      "TweetPreprocessed.\n",
      "(5254, 5)\n",
      "(1241, 5)\n",
      "(1400, 5)\n"
     ]
    }
   ],
   "source": [
    "chosenPath=pathB2\n",
    "ChosenType=TypeII\n",
    "print(chosenPath)\n",
    "print(ChosenType)\n",
    "\n",
    "df_train = pd.read_csv(path + chosenPath + ChosenType + \"csv\")\n",
    "df_train.dropna(subset = [\"tweet\",\"sarcastic\"], inplace=True)\n",
    "df_val = pd.read_csv(path + pathVal + ChosenType + \"csv\")\n",
    "df_val.dropna(subset = [\"tweet\",\"sarcastic\"], inplace=True)\n",
    "df_test = pd.read_csv(path + pathTest + ChosenType + \"csv\")\n",
    "df_test.dropna(subset = [\"tweet\",\"sarcastic\"], inplace=True)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>435</td>\n",
       "      <td>كان من الرائع اكتشاف امريكا ولكن كان اروع لو ل...</td>\n",
       "      <td>1</td>\n",
       "      <td>امريكا تشكل خطرا كبيرا على جميع الشعوب بما فيه...</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1268</td>\n",
       "      <td>كان رجل مسن منحني الظهر يسيرُ في الطريق .. فقا...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4137</td>\n",
       "      <td>\"يا جماعه هذا بوكيمون ماحدا عرف يصطاده ويطعميه...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>levant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4984</td>\n",
       "      <td>اسلحه امريكيه يمتلكها حزب الله المجوسي  من اي...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2036</td>\n",
       "      <td>\"RT @USER |           تبا جدار الصامتين       ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>181</td>\n",
       "      <td>بمناسبة إنّي مروحة قريبًا،مافيش حدّ يعرف وين ي...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>levant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5250</th>\n",
       "      <td>4983</td>\n",
       "      <td>لا لا ... سبعة ضيف عليهم السويد وهولاندا   ويط...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5251</th>\n",
       "      <td>327</td>\n",
       "      <td>\"RT @USER فيصل القاسم: بشار الأسد مستعد لبيع أ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <td>2972</td>\n",
       "      <td>قوات الأمن المصرية تقوم بضبط \"لاب توب\" يحوي أغ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5253</th>\n",
       "      <td>1674</td>\n",
       "      <td>ليه مينفعش حد يقدم بلاغ للإنتربول في وائل غنيم...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5254 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0          435  كان من الرائع اكتشاف امريكا ولكن كان اروع لو ل...          1   \n",
       "1         1268  كان رجل مسن منحني الظهر يسيرُ في الطريق .. فقا...          1   \n",
       "2         4137  \"يا جماعه هذا بوكيمون ماحدا عرف يصطاده ويطعميه...          1   \n",
       "3         4984   اسلحه امريكيه يمتلكها حزب الله المجوسي  من اي...          0   \n",
       "4         2036  \"RT @USER |           تبا جدار الصامتين       ...          0   \n",
       "...        ...                                                ...        ...   \n",
       "5249       181  بمناسبة إنّي مروحة قريبًا،مافيش حدّ يعرف وين ي...          0   \n",
       "5250      4983  لا لا ... سبعة ضيف عليهم السويد وهولاندا   ويط...          1   \n",
       "5251       327  \"RT @USER فيصل القاسم: بشار الأسد مستعد لبيع أ...          1   \n",
       "5252      2972  قوات الأمن المصرية تقوم بضبط \"لاب توب\" يحوي أغ...          1   \n",
       "5253      1674  ليه مينفعش حد يقدم بلاغ للإنتربول في وائل غنيم...          1   \n",
       "\n",
       "                                               rephrase dialect  \n",
       "0     امريكا تشكل خطرا كبيرا على جميع الشعوب بما فيه...     msa  \n",
       "1                                                   NaN     msa  \n",
       "2                                                   NaN  levant  \n",
       "3                                                   NaN     msa  \n",
       "4                                                   NaN     msa  \n",
       "...                                                 ...     ...  \n",
       "5249                                                NaN  levant  \n",
       "5250                                                NaN     msa  \n",
       "5251                                                NaN     msa  \n",
       "5252                                                NaN     msa  \n",
       "5253                                                NaN     msa  \n",
       "\n",
       "[5254 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2155</td>\n",
       "      <td>مراسل الميادين :جثث المختطفين  خالية الاحشاء ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1095</td>\n",
       "      <td>السعودية  الرياض  مكة  المدينة  الدرعية صورة ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1624</td>\n",
       "      <td>الأسواني:اذا لم تتوحد القوى الوطنية فورا من أج...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2434</td>\n",
       "      <td>تنور !</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2597</td>\n",
       "      <td>خلق جميل الشعور بالغير</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>2082</td>\n",
       "      <td>:small_blue_diamond:الثقافة القرآنية:radio_bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>777</td>\n",
       "      <td>ستبدأ بعد ساعه ونصف اول مناظره بين المرشحان ل...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>2977</td>\n",
       "      <td>ميسي يرفض مصافحة مدرب باريس سان جيرمان بعد است...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>2753</td>\n",
       "      <td>العنف والقتل في محيط مقر جماعة الإخوان مدان وم...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>357</td>\n",
       "      <td>ياللي بلاصص عضه اسد ولانظره حسد</td>\n",
       "      <td>1</td>\n",
       "      <td>توقف عن حسد غيرك</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1241 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0         2155   مراسل الميادين :جثث المختطفين  خالية الاحشاء ...          0   \n",
       "1         1095   السعودية  الرياض  مكة  المدينة  الدرعية صورة ...          0   \n",
       "2         1624  الأسواني:اذا لم تتوحد القوى الوطنية فورا من أج...          0   \n",
       "3         2434                                             تنور !          0   \n",
       "4         2597                             خلق جميل الشعور بالغير          0   \n",
       "...        ...                                                ...        ...   \n",
       "1236      2082   :small_blue_diamond:الثقافة القرآنية:radio_bu...          0   \n",
       "1237       777   ستبدأ بعد ساعه ونصف اول مناظره بين المرشحان ل...          0   \n",
       "1238      2977  ميسي يرفض مصافحة مدرب باريس سان جيرمان بعد است...          0   \n",
       "1239      2753  العنف والقتل في محيط مقر جماعة الإخوان مدان وم...          0   \n",
       "1240       357                    ياللي بلاصص عضه اسد ولانظره حسد          1   \n",
       "\n",
       "              rephrase dialect  \n",
       "0                  NaN     msa  \n",
       "1                  NaN     msa  \n",
       "2                  NaN     msa  \n",
       "3                  NaN    nile  \n",
       "4                  NaN     msa  \n",
       "...                ...     ...  \n",
       "1236               NaN     msa  \n",
       "1237               NaN     msa  \n",
       "1238               NaN    nile  \n",
       "1239               NaN     msa  \n",
       "1240  توقف عن حسد غيرك    nile  \n",
       "\n",
       "[1241 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>نتغير من الأفعال مانتغير من مزاجنا</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gulf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>هل هُناك صباح الخير بطريقة مختلفة؟</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>الهدرة عليا و المعاني على جارتي</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>اللهم إني أسألك حبَّكَ وحبَّ من يحبُّكَ وحبَّ ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>انسى الحياة والدنيا دى وتعالى نهرب منهم</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ياا عمريي صوت بنتهاا تهببلل</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>levant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>NaN</td>\n",
       "      <td>خنفسة شافت ولادها عالحيط قالت ده لولي وملضوم ف...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>NaN</td>\n",
       "      <td>يكذب بكل ثقه بالرغم من وضوح كذبه</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>NaN</td>\n",
       "      <td>بس أنت المهم عندي .</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>NaN</td>\n",
       "      <td>كلامك غير مفيد</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0          NaN                 نتغير من الأفعال مانتغير من مزاجنا          0   \n",
       "1          NaN                 هل هُناك صباح الخير بطريقة مختلفة؟          0   \n",
       "2          NaN                    الهدرة عليا و المعاني على جارتي          1   \n",
       "3          NaN  اللهم إني أسألك حبَّكَ وحبَّ من يحبُّكَ وحبَّ ...          0   \n",
       "4          NaN            انسى الحياة والدنيا دى وتعالى نهرب منهم          0   \n",
       "...        ...                                                ...        ...   \n",
       "1395       NaN                        ياا عمريي صوت بنتهاا تهببلل          0   \n",
       "1396       NaN  خنفسة شافت ولادها عالحيط قالت ده لولي وملضوم ف...          1   \n",
       "1397       NaN                   يكذب بكل ثقه بالرغم من وضوح كذبه          0   \n",
       "1398       NaN                                بس أنت المهم عندي .          0   \n",
       "1399       NaN                                     كلامك غير مفيد          0   \n",
       "\n",
       "      rephrase dialect  \n",
       "0          NaN    gulf  \n",
       "1          NaN    nile  \n",
       "2          NaN  magreb  \n",
       "3          NaN     msa  \n",
       "4          NaN    nile  \n",
       "...        ...     ...  \n",
       "1395       NaN  levant  \n",
       "1396       NaN    nile  \n",
       "1397       NaN    nile  \n",
       "1398       NaN    nile  \n",
       "1399       NaN  magreb  \n",
       "\n",
       "[1400 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "y3tY3ECJDPaz",
    "outputId": "b4ff4686-f568-4f3c-8eef-006485c6d660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 5254 sarcastic 3436 non sarcastic 1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_train.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_train.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_train.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val total 1241 sarcastic 298 non sarcastic 943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_val.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names);\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_val.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_val.sarcastic)\n",
    "print(\"Val total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test total 1400 sarcastic 200 non sarcastic 1200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_test.sarcastic)\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_test.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_test.sarcastic)\n",
    "print(\"test total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "\n",
    "ax.set_xticklabels(class_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'CAMeL-Lab/bert-base-arabic-camelbert-mix'\n",
    "bertweet = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvvcoU6nurHy"
   },
   "source": [
    "We have all building blocks required to create a PyTorch dataset. Let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPTweetDataset(Dataset):\n",
    "\n",
    "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "    self.tweets = tweets\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    tweet = str(self.tweets[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "      'tweet_text': tweet,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xz3ZOQXVPCwh",
    "outputId": "dd8d2844-3b22-425d-dc40-725f7f46e52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5254, 5), (1241, 5), (1400, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPTweetDataset(\n",
    "    tweets=df.tweet.to_numpy(),\n",
    "    targets=df.sarcastic.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vODDxMKsPHqI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Y93ldSN47FeT",
    "outputId": "ee6eaa1a-3f03-4e18-c059-02dbf8b8bc14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "IdU4YVqb7N8M",
    "outputId": "1f67fe37-6634-484f-caa2-1517e80a29d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SarcasmClassifier, self).__init__()\n",
    "    self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask, return_dict=False)\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mz7p__CqdaMO",
    "outputId": "7a933577-8c04-42f3-c3ea-ecb9c1c30a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) \n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "2rTCj46Zamry",
    "outputId": "04ecb643-ccda-461f-886f-aefe01f9a248",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3031, 0.6969],\n",
       "        [0.4864, 0.5136],\n",
       "        [0.3843, 0.6157],\n",
       "        [0.5242, 0.4758],\n",
       "        [0.5511, 0.4489],\n",
       "        [0.5053, 0.4947],\n",
       "        [0.5933, 0.4067],\n",
       "        [0.3430, 0.6570],\n",
       "        [0.5020, 0.4980],\n",
       "        [0.5766, 0.4234],\n",
       "        [0.4003, 0.5997],\n",
       "        [0.5064, 0.4936],\n",
       "        [0.4840, 0.5160],\n",
       "        [0.4907, 0.5093],\n",
       "        [0.4360, 0.5640],\n",
       "        [0.5195, 0.4805]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "#weights = [float(ns_t)/(s_t+ns_t),1.2*float(s_t)/(s_t+ns_t)] #as class distribution\n",
    "#class_weights = torch.FloatTensor(weights).cuda()\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)#CrossEntropyLoss() has already included a softmax layer inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4189098181152054 accuracy 0.819375713741911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.3209552365617874 accuracy 0.8605962933118453\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.19495696181892827 accuracy 0.9335744194899125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5999170479555733 accuracy 0.8251410153102336\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0697325078578026 accuracy 0.9809668823753331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.259952497235738 accuracy 0.7888799355358581\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.02954963410261606 accuracy 0.9918157594213933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.280138632527292 accuracy 0.8114423851732473\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.011218351240217294 accuracy 0.9973353635325467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.1859402842732005 accuracy 0.830781627719581\n",
      "\n",
      "CPU times: user 2min 21s, sys: 5.82 s, total: 2min 27s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "      )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "      )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jS3gJ_qBEljD",
    "outputId": "21f968b6-fd29-4e74-dee0-8dc9eacd301e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.645"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  tweet_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"tweet_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      tweet_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  val_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.9542    0.8165    0.8800       943\n",
      "    sarcastic     0.6014    0.8758    0.7131       298\n",
      "\n",
      "     accuracy                         0.8308      1241\n",
      "    macro avg     0.7778    0.8462    0.7966      1241\n",
      " weighted avg     0.8694    0.8308    0.8399      1241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zHdPZr60-0c_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.9607    0.6108    0.7468      1200\n",
      "    sarcastic     0.2669    0.8500    0.4062       200\n",
      "\n",
      "     accuracy                         0.6450      1400\n",
      "    macro avg     0.6138    0.7304    0.5765      1400\n",
      " weighted avg     0.8616    0.6450    0.6982      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "eea26f656b4850182355e5ba3607fa37b1d1e122add4b8b2e4fad1e2abcd3873"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
