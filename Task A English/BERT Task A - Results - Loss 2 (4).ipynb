{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. [Hugginface](https://huggingface.co/vinai/bertweet-large)\n",
    "2. [Sentiment Analysis with BERT](https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=PGnlRWvkY-2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['not_sarcastic', 'sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./\"\n",
    "\n",
    "TypeI=\"\"\n",
    "TypeII=\"TweetPreprocessed.\"\n",
    "TypeIII=\"HalfPreprocessed.\"\n",
    "TypeIV=\"FullyPreprocessed.\"\n",
    "\n",
    "pathO0=\"TaskA.En.train.NotAugmented.\"\n",
    "pathO1=\"TaskA.En.train.Augmented.\"\n",
    "pathO2=\"TaskA.En.train.Augmented.NotEmbedding.\"\n",
    "\n",
    "pathE0=\"TaskA.En.train.Augmented.NotBalanced.\"\n",
    "pathE1=\"TaskA.En.train.Augmented.Balanced.Original.\"\n",
    "pathE2=\"TaskA.En.train.Augmented.Balanced.NotOriginal.\"\n",
    "pathE3=\"TaskA.En.train.Augmented.Balanced.NotOriginal.NotEmbedding.\"\n",
    "\n",
    "\n",
    "pathB0=\"TaskA.En.train.Augmented.Biased0.Original.\"\n",
    "pathB1=\"TaskA.En.train.Augmented.Biased1.Original.\"\n",
    "pathB2=\"TaskA.En.train.Augmented.Biased2.Original.\"\n",
    "pathB3=\"TaskA.En.train.Augmented.Biased3.Original.\"\n",
    "pathB4=\"TaskA.En.train.Augmented.Biased4.Original.\"\n",
    "pathB5=\"TaskA.En.train.Augmented.Biased5.Original.\"\n",
    "pathB6=\"TaskA.En.train.Augmented.Biased6.Original.\"\n",
    "pathB7=\"TaskA.En.train.Augmented.Biased7.Original.\"\n",
    "pathB8=\"TaskA.En.train.Augmented.Biased8.Original.\"\n",
    "pathB9=\"TaskA.En.train.Augmented.Biased9.Original.\"\n",
    "\n",
    "pathVal=\"TaskA.En.Basic.Val.\"\n",
    "pathTest=\"TaskA.En.Basic.Test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskA.En.train.Augmented.Biased4.Original.\n",
      "TweetPreprocessed.\n",
      "(5158, 10)\n",
      "(1387, 10)\n",
      "(1400, 10)\n"
     ]
    }
   ],
   "source": [
    "chosenPath=pathB4\n",
    "ChosenType=TypeII\n",
    "print(chosenPath)\n",
    "print(ChosenType)\n",
    "\n",
    "df_train = pd.read_csv(path + chosenPath + ChosenType + \"csv\")\n",
    "df_train.dropna(subset = [\"tweet\",\"sarcastic\"], inplace=True)\n",
    "df_val = pd.read_csv(path + pathVal + ChosenType + \"csv\")\n",
    "df_val.dropna(subset = [\"tweet\",\"sarcastic\"], inplace=True)\n",
    "df_test = pd.read_csv(path + pathTest + ChosenType + \"csv\")\n",
    "df_test.dropna(subset = [\"tweet\",\"sarcastic\"], inplace=True)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3388</td>\n",
       "      <td>Whenever I get sad about how things are going ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>Today's the day, last to chance to sign to sho...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1750</td>\n",
       "      <td>@USER @USER @USER that’s how things like this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>959</td>\n",
       "      <td>#Dallas #traffic I sure missed you! #not @ Dow...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4834</td>\n",
       "      <td>not enough essays about hozier saying “i slith...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5153</th>\n",
       "      <td>1549</td>\n",
       "      <td>How this isn't just Cowboy Bark listed 100 tim...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5154</th>\n",
       "      <td>4081</td>\n",
       "      <td>@USER @USER The more liberal the city, the mor...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>919</td>\n",
       "      <td>stomach flu once again..what a surprise. #not ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>1134</td>\n",
       "      <td>Lutalo's back. Now would be a great time to an...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>3863</td>\n",
       "      <td>@USER @USER so basically the only storyline fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5158 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0         3388  Whenever I get sad about how things are going ...          0   \n",
       "1         2001  Today's the day, last to chance to sign to sho...          0   \n",
       "2         1750  @USER @USER @USER that’s how things like this ...          0   \n",
       "3          959  #Dallas #traffic I sure missed you! #not @ Dow...          1   \n",
       "4         4834  not enough essays about hozier saying “i slith...          0   \n",
       "...        ...                                                ...        ...   \n",
       "5153      1549  How this isn't just Cowboy Bark listed 100 tim...          0   \n",
       "5154      4081  @USER @USER The more liberal the city, the mor...          1   \n",
       "5155       919  stomach flu once again..what a surprise. #not ...          1   \n",
       "5156      1134  Lutalo's back. Now would be a great time to an...          0   \n",
       "5157      3863  @USER @USER so basically the only storyline fo...          1   \n",
       "\n",
       "     rephrase  sarcasm  irony  satire  understatement  overstatement  \\\n",
       "0         NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "1         NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "2         NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "3         NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "4         NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "...       ...      ...    ...     ...             ...            ...   \n",
       "5153      NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "5154      NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "5155      NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "5156      NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "5157      NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "\n",
       "      rhetorical_question  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "...                   ...  \n",
       "5153                  NaN  \n",
       "5154                  NaN  \n",
       "5155                  NaN  \n",
       "5156                  NaN  \n",
       "5157                  NaN  \n",
       "\n",
       "[5158 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2861</td>\n",
       "      <td>@USER @USER ppl really don't know what a joke ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>cold queso is so much better than warm queso H...</td>\n",
       "      <td>1</td>\n",
       "      <td>Cold queso is gross and warm queso is so much ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239</td>\n",
       "      <td>@USER @USER You are a right bundle of laughs t...</td>\n",
       "      <td>1</td>\n",
       "      <td>I would say he’s being miserable and not that ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2539</td>\n",
       "      <td>Calling all user researchers: who fancies beta...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230</td>\n",
       "      <td>@USER how has Marcel Kittle been allowed to us...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>660</td>\n",
       "      <td>imma still vote for vermin supreme this presid...</td>\n",
       "      <td>1</td>\n",
       "      <td>I will not be voting for Vermin Supreme this u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>2885</td>\n",
       "      <td>#NoMoreLockdowns #NoMoreLockdowns, not great. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>2440</td>\n",
       "      <td>Whenever I go to Starbucks I panic while order...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>133</td>\n",
       "      <td>@USER @USER @USER @USER @USER Finally someone ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Billionaires do not need anyone to speak up on...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>3092</td>\n",
       "      <td>After like 4 months I am finally over an emoti...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1387 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0         2861  @USER @USER ppl really don't know what a joke ...          0   \n",
       "1          155  cold queso is so much better than warm queso H...          1   \n",
       "2          239  @USER @USER You are a right bundle of laughs t...          1   \n",
       "3         2539  Calling all user researchers: who fancies beta...          0   \n",
       "4         3230  @USER how has Marcel Kittle been allowed to us...          0   \n",
       "...        ...                                                ...        ...   \n",
       "1382       660  imma still vote for vermin supreme this presid...          1   \n",
       "1383      2885  #NoMoreLockdowns #NoMoreLockdowns, not great. ...          0   \n",
       "1384      2440  Whenever I go to Starbucks I panic while order...          0   \n",
       "1385       133  @USER @USER @USER @USER @USER Finally someone ...          1   \n",
       "1386      3092  After like 4 months I am finally over an emoti...          0   \n",
       "\n",
       "                                               rephrase  sarcasm  irony  \\\n",
       "0                                                   NaN      NaN    NaN   \n",
       "1     Cold queso is gross and warm queso is so much ...      0.0    1.0   \n",
       "2     I would say he’s being miserable and not that ...      1.0    0.0   \n",
       "3                                                   NaN      NaN    NaN   \n",
       "4                                                   NaN      NaN    NaN   \n",
       "...                                                 ...      ...    ...   \n",
       "1382  I will not be voting for Vermin Supreme this u...      1.0    0.0   \n",
       "1383                                                NaN      NaN    NaN   \n",
       "1384                                                NaN      NaN    NaN   \n",
       "1385  Billionaires do not need anyone to speak up on...      1.0    0.0   \n",
       "1386                                                NaN      NaN    NaN   \n",
       "\n",
       "      satire  understatement  overstatement  rhetorical_question  \n",
       "0        NaN             NaN            NaN                  NaN  \n",
       "1        0.0             0.0            0.0                  0.0  \n",
       "2        0.0             0.0            0.0                  0.0  \n",
       "3        NaN             NaN            NaN                  NaN  \n",
       "4        NaN             NaN            NaN                  NaN  \n",
       "...      ...             ...            ...                  ...  \n",
       "1382     0.0             0.0            0.0                  0.0  \n",
       "1383     NaN             NaN            NaN                  NaN  \n",
       "1384     NaN             NaN            NaN                  NaN  \n",
       "1385     0.0             0.0            0.0                  0.0  \n",
       "1386     NaN             NaN            NaN                  NaN  \n",
       "\n",
       "[1387 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Pinball!</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>So the Scottish Government want people to get ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>villainous pro tip : change the device name on...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I would date any of these men :pleading_face:</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Omg how an earth is that a pen !! :clown_face:</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bringing Kanye and drake to a tl near you</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I love it when women are referred to as \"girl ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The fact that people still don't get that you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0          NaN  Size on the the Toulouse team, That pack is mo...          0   \n",
       "1          NaN                                           Pinball!          0   \n",
       "2          NaN  So the Scottish Government want people to get ...          1   \n",
       "3          NaN  villainous pro tip : change the device name on...          0   \n",
       "4          NaN      I would date any of these men :pleading_face:          0   \n",
       "...        ...                                                ...        ...   \n",
       "1395       NaN  I’ve just seen this and felt it deserved a Ret...          0   \n",
       "1396       NaN     Omg how an earth is that a pen !! :clown_face:          0   \n",
       "1397       NaN          Bringing Kanye and drake to a tl near you          0   \n",
       "1398       NaN  I love it when women are referred to as \"girl ...          1   \n",
       "1399       NaN  The fact that people still don't get that you ...          1   \n",
       "\n",
       "      rephrase  sarcasm  irony  satire  understatement  overstatement  \\\n",
       "0          NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "1          NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "2          NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "3          NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "4          NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "...        ...      ...    ...     ...             ...            ...   \n",
       "1395       NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "1396       NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "1397       NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "1398       NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "1399       NaN      NaN    NaN     NaN             NaN            NaN   \n",
       "\n",
       "      rhetorical_question  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "...                   ...  \n",
       "1395                  NaN  \n",
       "1396                  NaN  \n",
       "1397                  NaN  \n",
       "1398                  NaN  \n",
       "1399                  NaN  \n",
       "\n",
       "[1400 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "y3tY3ECJDPaz",
    "outputId": "b4ff4686-f568-4f3c-8eef-006485c6d660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 5158 sarcastic 3018 non sarcastic 2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXOUlEQVR4nO3dfbBddX3v8feHoBAEBErkxiQ21Im2gDVKSFHHik8l9taCD9zGq4KtNV4Ge7VjvQU7V7lt09oRa0WFDvgAeNtipqikHVExI9JeUDxwAyFgNBdQIhkIShVqGyV87x/rF9k92ck6gexzTnLer5k9e63vXmvt707OPp+zHvZvp6qQJGlX9pvqBiRJ059hIUnqZVhIknoZFpKkXoaFJKnX/lPdwKgceeSRtXDhwqluQ5L2KjfeeOP9VTVnfH2fDYuFCxcyNjY21W1I0l4lyXeG1T0MJUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF4jC4skBya5IcnNSdYn+V+tfkSSq5N8u90fPrDOOUk2JtmQ5OSB+vFJ1rXHzk+SUfUtSdrRKPcstgIvqapnA4uBZUlOBM4G1lTVImBNmyfJMcBy4FhgGXBBklltWxcCK4BF7bZshH1LksYZ2Se4q/tWpYfa7BParYBTgJNa/VLgGuAPW/3yqtoK3JlkI7A0yV3AoVV1PUCSy4BTgatG1bs03X33j5811S1oGnrae9aNbNsjPWeRZFaStcB9wNVV9XXgqKraDNDun9IWnwfcPbD6plab16bH14c934okY0nGtmzZskdfiyTNZCMNi6raVlWLgfl0ewnH7WLxYechahf1Yc93UVUtqaolc+bsMA6WJOkxmpSroarqX+gONy0D7k0yF6Dd39cW2wQsGFhtPnBPq88fUpckTZJRXg01J8lhbXo28DLgm8Bq4Iy22BnAlW16NbA8yQFJjqY7kX1DO1T1YJIT21VQpw+sI0maBKMconwucGm7omk/YFVV/WOS64FVSd4MfBc4DaCq1idZBdwGPAycVVXb2rbOBC4BZtOd2PbktiRNolFeDXUL8Jwh9e8DL93JOiuBlUPqY8CuzndIkkbIT3BLknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknqNLCySLEjylSS3J1mf5O2tfm6S7yVZ226/PrDOOUk2JtmQ5OSB+vFJ1rXHzk+SUfUtSdrR/iPc9sPAO6vqpiSHADcmubo99sGqOm9w4STHAMuBY4GnAl9O8oyq2gZcCKwAvgZ8HlgGXDXC3iVJA0a2Z1FVm6vqpjb9IHA7MG8Xq5wCXF5VW6vqTmAjsDTJXODQqrq+qgq4DDh1VH1LknY0KecskiwEngN8vZXeluSWJJ9IcnirzQPuHlhtU6vNa9Pj68OeZ0WSsSRjW7Zs2ZMvQZJmtJGHRZKDgSuAd1TVj+gOKT0dWAxsBj6wfdEhq9cu6jsWqy6qqiVVtWTOnDmPt3VJUjPSsEjyBLqg+Juq+gxAVd1bVduq6hHgYmBpW3wTsGBg9fnAPa0+f0hdkjRJRnk1VICPA7dX1V8O1OcOLPYq4NY2vRpYnuSAJEcDi4Abqmoz8GCSE9s2TweuHFXfkqQdjfJqqBcAbwTWJVnbau8GXpdkMd2hpLuAtwJU1fokq4Db6K6kOqtdCQVwJnAJMJvuKiivhJKkSTSysKiqf2b4+YbP72KdlcDKIfUx4Lg9150kaXf4CW5JUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1Gtl3cO/tjn/XZVPdgqahG99/+lS3IE0J9ywkSb0MC0lSL8NCktTLsJAk9TIsJEm9RhYWSRYk+UqS25OsT/L2Vj8iydVJvt3uDx9Y55wkG5NsSHLyQP34JOvaY+cnyaj6liTtaJR7Fg8D76yqXwJOBM5KcgxwNrCmqhYBa9o87bHlwLHAMuCCJLPati4EVgCL2m3ZCPuWJI0zsrCoqs1VdVObfhC4HZgHnAJc2ha7FDi1TZ8CXF5VW6vqTmAjsDTJXODQqrq+qgq4bGAdSdIkmJRzFkkWAs8Bvg4cVVWboQsU4CltsXnA3QOrbWq1eW16fF2SNElGHhZJDgauAN5RVT/a1aJDarWL+rDnWpFkLMnYli1bdr9ZSdJQIw2LJE+gC4q/qarPtPK97dAS7f6+Vt8ELBhYfT5wT6vPH1LfQVVdVFVLqmrJnDlz9twLkaQZbpRXQwX4OHB7Vf3lwEOrgTPa9BnAlQP15UkOSHI03YnsG9qhqgeTnNi2efrAOpKkSTDKgQRfALwRWJdkbau9G3gfsCrJm4HvAqcBVNX6JKuA2+iupDqrqra19c4ELgFmA1e1myRpkowsLKrqnxl+vgHgpTtZZyWwckh9DDhuz3UnSdodfoJbktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSrwmFRZI1E6lJkvZNuxx1NsmBwEHAkUkO59FRZA8Fnjri3iRJ00TfEOVvBd5BFww38mhY/Aj46OjakiRNJ7sMi6r6EPChJL9XVR+epJ4kSdPMhL78qKo+nOT5wMLBdarqshH1JUmaRiYUFkk+BTwdWAts/6rTAgwLSZoBJvq1qkuAY6qqRtmMJGl6mujnLG4F/tMoG5EkTV8T3bM4ErgtyQ3A1u3FqvrNkXQlSZpWJhoW546yCUnS9DbRq6G+OupGJEnT10SvhnqQ7uongCcCTwD+taoOHVVjkqTpY6J7FocMzic5FVg6ioYkSdPPYxp1tqo+B7xkz7YiSZquJjrq7KsHbq9N8j4ePSy1s3U+keS+JLcO1M5N8r0ka9vt1wceOyfJxiQbkpw8UD8+ybr22PlJMv65JEmjNdGroV45MP0wcBdwSs86lwAfYcdPeX+wqs4bLCQ5BlgOHEs3aOGXkzyjqrYBFwIrgK8BnweWAVdNsG9J0h4w0XMWv727G66qa5MsnODipwCXV9VW4M4kG4GlSe4CDq2q6wGSXAacimEhSZNqooeh5if5bDusdG+SK5LMf4zP+bYkt7TDVIe32jzg7oFlNrXavDY9vr6zPlckGUsytmXLlsfYniRpvIme4P4ksJruENE84B9abXddSDcg4WJgM/CBVh92HqJ2UR+qqi6qqiVVtWTOnDmPoT1J0jATDYs5VfXJqnq43S4Bdvu3cVXdW1XbquoR4GIevfx2E7BgYNH5wD2tPn9IXZI0iSYaFvcneUOSWe32BuD7u/tkSeYOzL6KboBC6PZalic5IMnRwCLghqraDDyY5MR2FdTpwJW7+7ySpMdnoldD/Q7dlU0fpDsMdB2wy5PeSf4OOInu+7s3Ae8FTkqyuG3jLrqvbaWq1idZBdxGd7XVWe1KKIAz6a6smk13YtuT25I0ySYaFn8CnFFVDwAkOQI4jy5Ehqqq1w0pf3wXy68EVg6pjwHHTbBPSdIITPQw1C9vDwqAqvoB8JzRtCRJmm4mGhb7DVzmun3PYqJ7JZKkvdxEf+F/ALguyd/TnW/4Lww5ZCRJ2jdN9BPclyUZoxs8MMCrq+q2kXYmSZo2JnwoqYWDASFJM9BjGqJckjSzGBaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqNbKwSPKJJPcluXWgdkSSq5N8u90fPvDYOUk2JtmQ5OSB+vFJ1rXHzk+SUfUsSRpulHsWlwDLxtXOBtZU1SJgTZsnyTHAcuDYts4FSWa1dS4EVgCL2m38NiVJIzaysKiqa4EfjCufAlzapi8FTh2oX15VW6vqTmAjsDTJXODQqrq+qgq4bGAdSdIkmexzFkdV1WaAdv+UVp8H3D2w3KZWm9emx9eHSrIiyViSsS1btuzRxiVpJpsuJ7iHnYeoXdSHqqqLqmpJVS2ZM2fOHmtOkma6yQ6Le9uhJdr9fa2+CVgwsNx84J5Wnz+kLkmaRJMdFquBM9r0GcCVA/XlSQ5IcjTdiewb2qGqB5Oc2K6COn1gHUnSJNl/VBtO8nfAScCRSTYB7wXeB6xK8mbgu8BpAFW1Pskq4DbgYeCsqtrWNnUm3ZVVs4Gr2k2SNIlGFhZV9bqdPPTSnSy/Elg5pD4GHLcHW5Mk7abpcoJbkjSNGRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6GRaSpF6GhSSpl2EhSeplWEiSehkWkqRehoUkqZdhIUnqZVhIknoZFpKkXoaFJKmXYSFJ6mVYSJJ6TUlYJLkryboka5OMtdoRSa5O8u12f/jA8uck2ZhkQ5KTp6JnSZrJpnLP4sVVtbiqlrT5s4E1VbUIWNPmSXIMsBw4FlgGXJBk1lQ0LEkz1XQ6DHUKcGmbvhQ4daB+eVVtrao7gY3A0slvT5JmrqkKiwK+lOTGJCta7aiq2gzQ7p/S6vOAuwfW3dRqkqRJsv8UPe8LquqeJE8Brk7yzV0smyG1GrpgFzwrAJ72tKc9/i4lScAU7VlU1T3t/j7gs3SHle5NMheg3d/XFt8ELBhYfT5wz062e1FVLamqJXPmzBlV+5I040x6WCR5UpJDtk8DvwbcCqwGzmiLnQFc2aZXA8uTHJDkaGARcMPkdi1JM9tUHIY6Cvhsku3P/7dV9YUk3wBWJXkz8F3gNICqWp9kFXAb8DBwVlVtm4K+JWnGmvSwqKo7gGcPqX8feOlO1lkJrBxxa5KknZhOl85KkqYpw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1MuwkCT1MiwkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLUy7CQJPUyLCRJvQwLSVIvw0KS1GuvCYsky5JsSLIxydlT3Y8kzSR7RVgkmQV8FHgFcAzwuiTHTG1XkjRz7BVhASwFNlbVHVX1E+By4JQp7kmSZoz9p7qBCZoH3D0wvwn4lfELJVkBrGizDyXZMAm9zQRHAvdPdRPTQc47Y6pb0I78+dzuvdkTW/n5YcW9JSyG/QvUDoWqi4CLRt/OzJJkrKqWTHUf0jD+fE6OveUw1CZgwcD8fOCeKepFkmacvSUsvgEsSnJ0kicCy4HVU9yTJM0Ye8VhqKp6OMnbgC8Cs4BPVNX6KW5rJvHQnqYzfz4nQap2OPQvSdJ/sLcchpIkTSHDQpLUy7CQNCMkefe4+eumqpe9kWGxD0rypiRPneo+dmV8j0k+5hAuGibJnroQ5z+ERVU9fw9td0bYK66G0m57E3ArI/gsSpL9q+rhPbCpNzHQY1X97h7YpqaxJE8CVtF9TmoW8CfAM4FXArOB64C3VlUluabNvwBYneRa4EPAk4CtwEuBnwM+1WoAb6uq65LMBT4NHEr3O+5M4D8Ds5OsBdZX1euTPFRVB7fe/gfwRuAR4KqqcrDS8arK2zS/AQuB24GLgfXAl+jeXIuBrwG3AJ8FDgdeCzwEbADWArN3ss33Abe1dc9rtVcCXwf+L/Bl4KhWP5fu8sQvAX8LHNWe7+Z2e35b7nPAja3HFa02C7iELhjWAb8/rEfgGmBJW2cZcFPb9pqp/vf3tsd+jl8DXDww/2TgiIH5TwGvbNPXABe06ScCdwAntPntIXAQcGCrLQLG2vQ7gT8a+Pk7pE0/NK6fh9r9K+iC6aA2f8Tjfa374m3KG/A2gf+kLiweBha3+VXAG9ov+he12h8Df9Wmf/aLdyfbO6L9ot5+6fRh7f7wgdrvAh9o0+e2EJjd5j8NvKNNzwKevH277X52C4efA44Hrh547sOG9bh9HphDNw7Y0YPb9Lb334BnAHcCfwG8sNVeQ/cHyjrge8DZAz8PL2rTzwL+z5DtPbkFzDq6Pzp+3Oq/CmxsP7eLB5bfWVh8AHjLVP/7TPeb5yz2HndW1do2fSPwdLpfvF9ttUvp3iQT8SPg34GPJXk18ONWnw98Mck64F3AsQPrrK6qf2vTLwEuBKiqbVX1w1b/70luptvbWUD3194dwC8k+XCSZe25d+VE4NqqurNt/wcTfE2a5qrqW3R/PKwD/jzJe4ALgNdW1bPo9pwPHFjlX9t9GDIWHN1e6r3As+n+0Hhie55r6d4L3wM+leT0ntZ2tn0NMCz2HlsHprcBhz3WDVV3zmEpcAVwKvCF9tCHgY+0N+5bGf7GHSrJScDLgOdV1bPpDmUdWFUP0L2ZrwHOAj7W055v3H1Uu6Dhx1X1v4HzgOe2h+5PcjDd4clhvgk8NckJbTuHtJPeTwY2V9UjdOcbZrXHfx64r6ouBj4+8Dw/TfKEIdv/EvA7SQ5q6x/xOF/qPskT3HuvHwIPJHlhVf0T3Ztl+17Gg8AhO1uxvTEPqqrPJ/ka3S47dG++77XpXY3FvYbupOFftS+melJb94Gq+nGSX6TbQyDJkcBPquqKJP+P7vzFrnq8HvhokqOr6s4kR7h3sc94FvD+JI8AP6X7GTqVbk/jLrox4HZQVT9J8lvAh5PMBv6N7g+TC4ArkpwGfIVH/6A5CXhXkp/SnRvbvmdxEXBLkpuq6vUD2/9CksXAWJKfAJ9n3JVTcriPvUKShcA/VtVxbf4PgIPpTij/Nd2JvjuA366qB5K8BvgzujfV8wYOH23f3lzgSro9h9Cd4L40ySnAB+kC42t0JxRPSnIu3fHd89r6R9G98X6Bbi/nTLoT0p+j++6RDXTnHs4FHgA+yaN7sedU1VXjewSuAv6gqsaSvKI9th/dX4gvf/z/ipIeD8NCktTLcxaSpF6es9jHJfkscPS48h9W1Renoh9JeycPQ0mSenkYSpLUy7CQJPUyLDQjJPmjJOuT3JJkbZJfmeqe+iRZmOTW3Vj+kiQ7+2Db496+ZjZPcGufl+R5wG8Az62qre2Dgk/cjfX31Ei70l7LPQvNBHOB+6tqK0BV3V9V9wAkeU+SbyS5NclFSdLq1yT5syRfBd6e5IQk1yW5OckNbciJhUn+KclN7fb8tu7cJNe2PZhbk7yw1R9K8hdJbkzy5SRL2/PckeQ3J/pikryl9Xxzkiu2D1PRvKz19K0kv9GWn5Xk/W2dW5K8dY/8q2pGMSw0E3wJWNB+gV6Q5EUDj32kqk5on46fTbcHst1hVfUiujGzPg28vY179TK6T57fB7y8qp4L/BZwflvvvwJfrKrFdONirW31JwHXVNXxdMOd/CnwcuBVdKMGT9RnWs/Pphu6/s0Djy0EXkT3/Q1/neTA9vgPq+oE4ATgLUnGX04t7ZKHobTPq6qHkhwPvBB4MfDpJGdX1SXAi9sX3xxEN3T7euAf2qqfbvfPpBuw7httez+Cn32Zz0fauELb6Ibghm6Mo0+0Qes+NzBa8E94dNDGdcDWqvppG+V34W68pOOS/CndYJIHA4OfmVnVBtb7dpI7gF8Efg345YHzGU+mGxH4W7vxnJrhDAvNCFW1jW7k22vaL+czklxONxjdkqq6u42B9ViHyN6Pbth3quraJL9K99f9p5K8v6ouA35aj36w6RHaSMJV9Uh276tDLwFOraqbk7yJbuC8n73U8S+99f974z+I2cYckybEw1Da5yV5ZpJFA6XFwHd4NBgma4jsPeUQYHPbc3n9uMdOS7JfkqfTDfS4gW7P48ztw3MneUbbK5ImzD0LzQQH0w1vfRjdNw5upPva139JcjGTN0T2Y/HMJJsG5n8f+J903y73ndb74FDvG+iGqj8K+G9V9e9JPkZ3mOumdgJ/C93Q4NKEOdyHJKmXh6EkSb0MC0lSL8NCktTLsJAk9TIsJEm9DAtJUi/DQpLU6/8Dvufyz+QB1DIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_train.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "s_train = 0\n",
    "ns_train =0\n",
    "for i in df_train.sarcastic:\n",
    "    if i == 1:\n",
    "        s_train+=1\n",
    "    else:\n",
    "        ns_train+=1\n",
    "l_count = len(df_train.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", s_train, \"non sarcastic\", ns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val total 1387 sarcastic 347 non sarcastic 1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_val.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names);\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_val.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_val.sarcastic)\n",
    "print(\"Val total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test total 1400 sarcastic 200 non sarcastic 1200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_test.sarcastic)\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_test.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_test.sarcastic)\n",
    "print(\"test total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "\n",
    "ax.set_xticklabels(class_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'vinai/bertweet-large'\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPTweetDataset(Dataset):\n",
    "\n",
    "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "    self.tweets = tweets\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    tweet = str(self.tweets[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "      'tweet_text': tweet,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xz3ZOQXVPCwh",
    "outputId": "dd8d2844-3b22-425d-dc40-725f7f46e52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5158, 10), (1387, 10), (1400, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPTweetDataset(\n",
    "    tweets=df.tweet.to_numpy(),\n",
    "    targets=df.sarcastic.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vODDxMKsPHqI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Y93ldSN47FeT",
    "outputId": "ee6eaa1a-3f03-4e18-c059-02dbf8b8bc14",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "IdU4YVqb7N8M",
    "outputId": "1f67fe37-6634-484f-caa2-1517e80a29d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SarcasmClassifier, self).__init__()\n",
    "    self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask, return_dict=False)\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mz7p__CqdaMO",
    "outputId": "7a933577-8c04-42f3-c3ea-ecb9c1c30a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "2rTCj46Zamry",
    "outputId": "04ecb643-ccda-461f-886f-aefe01f9a248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6568, 0.3432],\n",
       "        [0.5508, 0.4492],\n",
       "        [0.6148, 0.3852],\n",
       "        [0.5293, 0.4707],\n",
       "        [0.5263, 0.4737],\n",
       "        [0.4686, 0.5314],\n",
       "        [0.5482, 0.4518],\n",
       "        [0.6500, 0.3500],\n",
       "        [0.5778, 0.4222],\n",
       "        [0.6186, 0.3814],\n",
       "        [0.6288, 0.3712],\n",
       "        [0.6098, 0.3902],\n",
       "        [0.5817, 0.4183],\n",
       "        [0.4899, 0.5101],\n",
       "        [0.5965, 0.4035],\n",
       "        [0.6126, 0.3874]], device='cuda:1', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=4e-6, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "#weights = [float(s_train)/(s_train+ns_train),float(ns_train)/(s_train+ns_train)] #as class distribution\n",
    "weights = [float(1.0)/(ns_train),float(1.0)/(s_train)]\n",
    "#weights = [float(1.0)/(math.sqrt(ns_train)),float(1.0)/(math.sqrt(s_train))]\n",
    "# B=0.99\n",
    "# E1=(1-pow(B, ns_train))/(1-B)\n",
    "# w1=1/E1\n",
    "# E2=(1-pow(B, s_train))/(1-B)\n",
    "# w2=1/E2\n",
    "# weights = [w1,w2]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to(device)\n",
    "#loss_fn = nn.CrossEntropyLoss().to(device)#CrossEntropyLoss() has already included a softmax layer inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.44206491214203025 accuracy 0.7842186894145017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.39862630893101636 accuracy 0.8103821196827685\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.28342561133425026 accuracy 0.8846452113222179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.47864541831030244 accuracy 0.8175919250180245\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.20306294288229623 accuracy 0.9286545172547499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.7008786642142496 accuracy 0.8010093727469358\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.16230754973907485 accuracy 0.9530825901512213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.8684981801351747 accuracy 0.8067772170151406\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.12728774759088768 accuracy 0.9641333850329584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 1.0118278869609723 accuracy 0.7916366258111031\n",
      "\n",
      "CPU times: user 11min 58s, sys: 2min 51s, total: 14min 49s\n",
      "Wall time: 14min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "      )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "      )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jS3gJ_qBEljD",
    "outputId": "21f968b6-fd29-4e74-dee0-8dc9eacd301e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7571428571428571"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  tweet_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"tweet_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      tweet_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  val_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.8729    0.8452    0.8588      1040\n",
      "    sarcastic     0.5763    0.6311    0.6025       347\n",
      "\n",
      "     accuracy                         0.7916      1387\n",
      "    macro avg     0.7246    0.7382    0.7306      1387\n",
      " weighted avg     0.7987    0.7916    0.7947      1387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zHdPZr60-0c_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.9526    0.7542    0.8419      1200\n",
      "    sarcastic     0.3444    0.7750    0.4769       200\n",
      "\n",
      "     accuracy                         0.7571      1400\n",
      "    macro avg     0.6485    0.7646    0.6594      1400\n",
      " weighted avg     0.8657    0.7571    0.7897      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "eea26f656b4850182355e5ba3607fa37b1d1e122add4b8b2e4fad1e2abcd3873"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
