{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. [Hugginface](https://huggingface.co/vinai/bertweet-large)\n",
    "2. [Sentiment Analysis with BERT](https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=PGnlRWvkY-2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V-155O-SFSqE"
   },
   "outputs": [],
   "source": [
    "label_names = ['sarcasm', 'irony','satire', 'understatement','overstatement', 'rhetorical_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./\"\n",
    "\n",
    "\n",
    "TypeI=\"\"\n",
    "TypeII=\"TweetPreprocessed.\"\n",
    "TypeIII=\"HalfPreprocessed.\"\n",
    "TypeIV=\"FullyPreprocessed.\"\n",
    "\n",
    "path0=\"TaskB.En.train.Augmented.NotBalanced.\"\n",
    "path1=\"TaskB.En.train.Augmented.Embedding.\"\n",
    "path2=\"TaskB.En.train.Augmented.NotEmbedding.\"\n",
    "path3=\"TaskB.En.train.Augmented.BiasedToLimit.\"\n",
    "\n",
    "pathVal=\"TaskB.En.Basic.Val.\"\n",
    "pathTest=\"TaskB.En.Basic.Test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskB.En.train.Augmented.BiasedToLimit.\n",
      "TweetPreprocessed.\n",
      "(4805, 10)\n",
      "(522, 10)\n",
      "(1400, 10)\n"
     ]
    }
   ],
   "source": [
    "chosenPath=path3\n",
    "ChosenType=TypeII\n",
    "print(chosenPath)\n",
    "print(ChosenType)\n",
    "\n",
    "df_train = pd.read_csv(path + chosenPath + ChosenType + \"csv\")\n",
    "df_train.dropna(subset = [\"tweet\"], inplace=True)\n",
    "df_val = pd.read_csv(path + pathVal + ChosenType + \"csv\")\n",
    "df_val.dropna(subset = [\"tweet\"], inplace=True)\n",
    "df_test = pd.read_csv(path + pathTest + ChosenType + \"csv\")\n",
    "df_test.dropna(subset = [\"tweet\"], inplace=True)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "y3tY3ECJDPaz",
    "outputId": "b4ff4686-f568-4f3c-8eef-006485c6d660"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>saw a video of someone getting a hug. would LO...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\"This Christmas I hope you all either get vacc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>It's the alamo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Wind 5 mph E. Barometer 1029.8 hPa, Pressure t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I constantly have loads of the new symptoms bu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Tempting to renew my membership and vote again...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>NaN</td>\n",
       "      <td>This week has felt like the longest in history...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Of course it’s raining when I’m due to go out ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Weigh up a lie before you tell it.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Upand dressed at a reasonable time once again ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0          NaN  saw a video of someone getting a hug. would LO...        NaN   \n",
       "1          NaN  \"This Christmas I hope you all either get vacc...        NaN   \n",
       "2          NaN                                     It's the alamo        NaN   \n",
       "3          NaN  Wind 5 mph E. Barometer 1029.8 hPa, Pressure t...        NaN   \n",
       "4          NaN  I constantly have loads of the new symptoms bu...        NaN   \n",
       "...        ...                                                ...        ...   \n",
       "1395       NaN  Tempting to renew my membership and vote again...        NaN   \n",
       "1396       NaN  This week has felt like the longest in history...        NaN   \n",
       "1397       NaN  Of course it’s raining when I’m due to go out ...        NaN   \n",
       "1398       NaN                 Weigh up a lie before you tell it.        NaN   \n",
       "1399       NaN  Upand dressed at a reasonable time once again ...        NaN   \n",
       "\n",
       "      rephrase  sarcasm  irony  satire  understatement  overstatement  \\\n",
       "0          NaN        0      0       0               0              0   \n",
       "1          NaN        0      0       0               0              0   \n",
       "2          NaN        0      0       0               0              0   \n",
       "3          NaN        0      0       0               0              0   \n",
       "4          NaN        0      0       0               0              0   \n",
       "...        ...      ...    ...     ...             ...            ...   \n",
       "1395       NaN        0      0       0               0              0   \n",
       "1396       NaN        0      0       0               0              0   \n",
       "1397       NaN        0      0       0               0              0   \n",
       "1398       NaN        0      0       0               0              0   \n",
       "1399       NaN        0      0       0               0              0   \n",
       "\n",
       "      rhetorical_question  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "1395                    0  \n",
       "1396                    0  \n",
       "1397                    0  \n",
       "1398                    0  \n",
       "1399                    0  \n",
       "\n",
       "[1400 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 20 49 1 10 11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarcasm=df_test['sarcasm'].sum()\n",
    "irony=df_test['irony'].sum()\n",
    "satire=df_test['satire'].sum()\n",
    "understatement=df_test['understatement'].sum()\n",
    "overstatement=df_test['overstatement'].sum()\n",
    "rhetorical_question=df_test['rhetorical_question'].sum()\n",
    "print(sarcasm,irony,satire,understatement,overstatement,rhetorical_question)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1.5,1.5])\n",
    "langs = ['sarcasm', 'irony', 'satire', 'understatement', 'overstatement', 'rhetorical_question']\n",
    "students = [sarcasm,irony,satire,understatement,overstatement,rhetorical_question]\n",
    "ax.bar(langs,students)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744.0 788.0 829.0 933.0 844.0 835.0\n"
     ]
    }
   ],
   "source": [
    "sarcasm=df_train['sarcasm'].sum()\n",
    "irony=df_train['irony'].sum()\n",
    "satire=df_train['satire'].sum()\n",
    "understatement=df_train['understatement'].sum()\n",
    "overstatement=df_train['overstatement'].sum()\n",
    "rhetorical_question=df_train['rhetorical_question'].sum()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1.5,1.5])\n",
    "langs = ['sarcasm', 'irony', 'satire', 'understatement', 'overstatement', 'rhetorical_question']\n",
    "students = [sarcasm,irony,satire,understatement,overstatement,rhetorical_question]\n",
    "ax.bar(langs,students)\n",
    "plt.show()\n",
    "print(sarcasm,irony,satire,understatement,overstatement,rhetorical_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'vinai/bertweet-large'\n",
    "bertweet = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPTweetDataset(Dataset):\n",
    "\n",
    "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "    self.tweets = tweets\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    tweet = str(self.tweets[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "      'tweet_text': tweet,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xz3ZOQXVPCwh",
    "outputId": "dd8d2844-3b22-425d-dc40-725f7f46e52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4805, 10), (522, 10), (1400, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "We also need to create a couple of data loaders. Here's a helper function to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  \n",
    "  sarcasm = df.sarcasm.to_numpy()\n",
    "  irony = df.irony.to_numpy()\n",
    "  satire = df.satire.to_numpy()\n",
    "  understatement = df.understatement.to_numpy()\n",
    "  overstatement = df.overstatement.to_numpy()\n",
    "  rhetorical_question = df.rhetorical_question.to_numpy()\n",
    "\n",
    "  labels = df[[\"sarcasm\", \"irony\",\"satire\", \"understatement\",\"overstatement\", \"rhetorical_question\"]]\n",
    "  labels = labels.to_numpy()\n",
    "    \n",
    "  ds = GPTweetDataset(\n",
    "    tweets=df.tweet.to_numpy(),\n",
    "    targets=labels,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vODDxMKsPHqI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Y93ldSN47FeT",
    "outputId": "ee6eaa1a-3f03-4e18-c059-02dbf8b8bc14",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "IdU4YVqb7N8M",
    "outputId": "1f67fe37-6634-484f-caa2-1517e80a29d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n",
      "torch.Size([16, 6])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SarcasmClassifier, self).__init__()\n",
    "    self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask, return_dict=False)\n",
    "    output = self.drop(pooled_output)\n",
    "    output = self.out(output)\n",
    "    return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(label_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mz7p__CqdaMO",
    "outputId": "7a933577-8c04-42f3-c3ea-ecb9c1c30a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "2rTCj46Zamry",
    "outputId": "04ecb643-ccda-461f-886f-aefe01f9a248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100])\n",
      "torch.Size([16, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1751, 0.1546, 0.1566, 0.1590, 0.1766, 0.1781],\n",
       "        [0.1612, 0.1544, 0.1733, 0.1684, 0.1687, 0.1740],\n",
       "        [0.1608, 0.1527, 0.1578, 0.1616, 0.1829, 0.1842],\n",
       "        [0.1567, 0.1540, 0.1762, 0.1646, 0.1798, 0.1687],\n",
       "        [0.1539, 0.1545, 0.1564, 0.1657, 0.1750, 0.1946],\n",
       "        [0.1601, 0.1655, 0.1666, 0.1584, 0.1733, 0.1760],\n",
       "        [0.1661, 0.1583, 0.1615, 0.1635, 0.1689, 0.1818],\n",
       "        [0.1686, 0.1529, 0.1635, 0.1633, 0.1788, 0.1728],\n",
       "        [0.1658, 0.1557, 0.1563, 0.1691, 0.1824, 0.1707],\n",
       "        [0.1705, 0.1459, 0.1689, 0.1642, 0.1697, 0.1809],\n",
       "        [0.1654, 0.1496, 0.1603, 0.1703, 0.1770, 0.1775],\n",
       "        [0.1580, 0.1442, 0.1600, 0.1712, 0.1825, 0.1841],\n",
       "        [0.1680, 0.1565, 0.1651, 0.1665, 0.1691, 0.1748],\n",
       "        [0.1589, 0.1634, 0.1604, 0.1672, 0.1707, 0.1794],\n",
       "        [0.1691, 0.1528, 0.1632, 0.1564, 0.1804, 0.1781],\n",
       "        [0.1670, 0.1540, 0.1614, 0.1641, 0.1709, 0.1826]], device='cuda:1',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=6e-6, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    \n",
    "    preds = outputs.round()\n",
    "\n",
    "    targets = targets.to(torch.float32)\n",
    "\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    \n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / (n_examples*len(label_names)), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "    \n",
    "      preds = outputs.round()\n",
    "\n",
    "      targets = targets.to(torch.float32)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "        \n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / (n_examples*len(label_names)), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.2697092326500289 accuracy 0.8950398890045091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.41269215218948596 accuracy 0.7598978288633461\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.09347497184832032 accuracy 0.9655913978494622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.34163933360215387 accuracy 0.8611111111111112\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.05164408488848875 accuracy 0.9858827610128338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.408160409692562 accuracy 0.8607918263090677\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0322492325622774 accuracy 0.9932015261879985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.40530412982810626 accuracy 0.8764367816091955\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.023918448542043617 accuracy 0.9956295525494276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.3893985499938329 accuracy 0.884418901660281\n",
      "\n",
      "CPU times: user 10min 39s, sys: 2min 40s, total: 13min 19s\n",
      "Wall time: 13min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "      )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "      )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jS3gJ_qBEljD",
    "outputId": "21f968b6-fd29-4e74-dee0-8dc9eacd301e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8329761904761904"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  tweet_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"tweet_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      preds = outputs.round()\n",
    "      probs = outputs\n",
    "\n",
    "      tweet_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  val_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            sarcasm     0.8883    0.7696    0.8247       434\n",
      "              irony     0.3217    0.5227    0.3983        88\n",
      "             satire     0.4286    0.2308    0.3000        13\n",
      "     understatement     0.0000    0.0000    0.0000         6\n",
      "      overstatement     0.8889    0.2667    0.4103        30\n",
      "rhetorical_question     0.6857    0.4706    0.5581        51\n",
      "\n",
      "          micro avg     0.7281    0.6672    0.6963       622\n",
      "          macro avg     0.5355    0.3767    0.4152       622\n",
      "       weighted avg     0.7734    0.6672    0.7036       622\n",
      "        samples avg     0.7257    0.6906    0.6975       622\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "zHdPZr60-0c_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            sarcasm     0.1335    0.8333    0.2301       180\n",
      "              irony     0.0708    0.7500    0.1293        20\n",
      "             satire     0.0345    0.0204    0.0256        49\n",
      "     understatement     0.0000    0.0000    0.0000         1\n",
      "      overstatement     0.0000    0.0000    0.0000        10\n",
      "rhetorical_question     0.0610    0.4545    0.1075        11\n",
      "\n",
      "          micro avg     0.1160    0.6310    0.1960       271\n",
      "          macro avg     0.0499    0.3430    0.0821       271\n",
      "       weighted avg     0.1026    0.6310    0.1714       271\n",
      "        samples avg     0.1175    0.0987    0.1046       271\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=label_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "eea26f656b4850182355e5ba3607fa37b1d1e122add4b8b2e4fad1e2abcd3873"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
