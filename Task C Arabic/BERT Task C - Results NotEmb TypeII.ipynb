{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. [Hugginface](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix)\n",
    "2. [Sentiment Analysis with BERT](https://colab.research.google.com/drive/1PHv-IRLPCtv7oTcIGbsgZHqrB5LPvB7S#scrollTo=PGnlRWvkY-2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "w68CZpOwFoly",
    "outputId": "9c1a0321-1650-4224-cf9c-3c8dc8661ed3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['not_sarcastic', 'sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./\"\n",
    "\n",
    "TypeI=\"\"\n",
    "TypeII=\"TweetPreprocessed.\"\n",
    "TypeIII=\"HalfPreprocessed.\"\n",
    "TypeIV=\"FullyPreprocessed.\"\n",
    "\n",
    "path0=\"TaskC.Ar.train.NotAugmented.\"\n",
    "path1=\"TaskC.Ar.train.Augmented.Embedding.\"\n",
    "path2=\"TaskC.Ar.train.Augmented.NotEmbedding.\"\n",
    "\n",
    "pathVal=\"TaskC.Ar.Basic.Val.\"\n",
    "pathTest=\"TaskC.Ar.Basic.Test.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskC.Ar.train.Augmented.NotEmbedding.\n",
      "TweetPreprocessed.\n",
      "(1521, 5)\n",
      "(298, 5)\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "chosenPath=path2\n",
    "ChosenType=TypeII\n",
    "print(chosenPath)\n",
    "print(ChosenType)\n",
    "\n",
    "df_train = pd.read_csv(path + chosenPath + ChosenType + \"csv\")\n",
    "df_train.dropna(subset = [\"tweet\"], inplace=True)\n",
    "df_val = pd.read_csv(path + pathVal + ChosenType + \"csv\")\n",
    "df_val.dropna(subset = [\"tweet\"], inplace=True)\n",
    "df_test = pd.read_csv(path + pathTest + ChosenType + \"csv\")\n",
    "df_test.dropna(subset = [\"tweet\"], inplace=True)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279.0</td>\n",
       "      <td>الأيفون غالى ومش هعرف أجيبه غير لما موديله يقدم</td>\n",
       "      <td>0</td>\n",
       "      <td>الحمدلله ايفون 8 كدا سعره هينزل ابقي اجيبه بقي</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>الوضع لوز</td>\n",
       "      <td>1</td>\n",
       "      <td>الوضع ممتاز</td>\n",
       "      <td>levant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>مؤكد العربية ديه حتنطلق للخلف بتوجيهاتكم</td>\n",
       "      <td>1</td>\n",
       "      <td>لن تستطيعوا تصنيع السيارة لانكم فاشلون جدا</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>صاحبي ده يقولي انا خلاص دقيقتن واكون قدامك فعل...</td>\n",
       "      <td>1</td>\n",
       "      <td>بيكون نايم ف السرير لسه ويقولي دقيقتين وهكون ق...</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>714.0</td>\n",
       "      <td>عايزين تعرفوا الشروط هوا شرط واحد يستخدم فى كل...</td>\n",
       "      <td>1</td>\n",
       "      <td>الواسطة هي اهم شيء للعمل</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>NaN</td>\n",
       "      <td>مستنين شهر أبريل عشان يكذبون فيه على أساس أنهم...</td>\n",
       "      <td>1</td>\n",
       "      <td>هما مش فارق معاهم اى شهر فى السنه بيكذبو على طول</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>NaN</td>\n",
       "      <td>مفيش راجل بيدافع عن حد دلوقتى</td>\n",
       "      <td>0</td>\n",
       "      <td>الرجاله ماتت في الحرب</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>11.0</td>\n",
       "      <td>سنة أغسطس بيكون فيها فجوة زمنية مجهولة الأيام ...</td>\n",
       "      <td>1</td>\n",
       "      <td>شهر اغسطس يبدو طويلا جدا</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>245.0</td>\n",
       "      <td>الشعب المغربي يركع لغير الله و فرحان</td>\n",
       "      <td>0</td>\n",
       "      <td>حنا الماليك تعنا يعطينا البنان عاش الماليك هههه</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ده عاملى فيها ابو العريف</td>\n",
       "      <td>1</td>\n",
       "      <td>ده غبى لكن فاهم نفسه عارف كل حاجه زى ابو المعرفة</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1521 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sarcastic  \\\n",
       "0        279.0    الأيفون غالى ومش هعرف أجيبه غير لما موديله يقدم          0   \n",
       "1          NaN                                          الوضع لوز          1   \n",
       "2          NaN           مؤكد العربية ديه حتنطلق للخلف بتوجيهاتكم          1   \n",
       "3          NaN  صاحبي ده يقولي انا خلاص دقيقتن واكون قدامك فعل...          1   \n",
       "4        714.0  عايزين تعرفوا الشروط هوا شرط واحد يستخدم فى كل...          1   \n",
       "...        ...                                                ...        ...   \n",
       "1516       NaN  مستنين شهر أبريل عشان يكذبون فيه على أساس أنهم...          1   \n",
       "1517       NaN                      مفيش راجل بيدافع عن حد دلوقتى          0   \n",
       "1518      11.0  سنة أغسطس بيكون فيها فجوة زمنية مجهولة الأيام ...          1   \n",
       "1519     245.0               الشعب المغربي يركع لغير الله و فرحان          0   \n",
       "1520       NaN                           ده عاملى فيها ابو العريف          1   \n",
       "\n",
       "                                               rephrase dialect  \n",
       "0        الحمدلله ايفون 8 كدا سعره هينزل ابقي اجيبه بقي    nile  \n",
       "1                                           الوضع ممتاز  levant  \n",
       "2            لن تستطيعوا تصنيع السيارة لانكم فاشلون جدا    nile  \n",
       "3     بيكون نايم ف السرير لسه ويقولي دقيقتين وهكون ق...    nile  \n",
       "4                              الواسطة هي اهم شيء للعمل    nile  \n",
       "...                                                 ...     ...  \n",
       "1516   هما مش فارق معاهم اى شهر فى السنه بيكذبو على طول    nile  \n",
       "1517                              الرجاله ماتت في الحرب    nile  \n",
       "1518                           شهر اغسطس يبدو طويلا جدا    nile  \n",
       "1519    حنا الماليك تعنا يعطينا البنان عاش الماليك هههه  magreb  \n",
       "1520   ده غبى لكن فاهم نفسه عارف كل حاجه زى ابو المعرفة    nile  \n",
       "\n",
       "[1521 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>الدنيا عاملة زى اوضة وصالة كل ما امشى فى حته ا...</td>\n",
       "      <td>1</td>\n",
       "      <td>الدنيا صغيرة ودايما نلاقى فيها حد نعرفه</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>218</td>\n",
       "      <td>لا وانت الشهادة لله وفي اوي</td>\n",
       "      <td>1</td>\n",
       "      <td>انت خائن</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>من قير كوكا لا تحدثني عل قاز هذا راه بيت شعري</td>\n",
       "      <td>1</td>\n",
       "      <td>كوكاكولا أفضل مشروب</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>السعودية : لا أعرفه ، لذا لن أضيفه .المصرية :ا...</td>\n",
       "      <td>1</td>\n",
       "      <td>ان المصريين بيضيفوا ناس كتير على مواقع التواصل</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>244</td>\n",
       "      <td>ساصدق قزانة الدشرة و مانامنش اتصالات الجزائر ا...</td>\n",
       "      <td>1</td>\n",
       "      <td>أتصالات الجزائر كادبين</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>6</td>\n",
       "      <td>الدنيا عاملة زى اوضة وصالة كل ما امشى فى حته ا...</td>\n",
       "      <td>1</td>\n",
       "      <td>الدنيا صغيرة ودايما نلاقى فيها حد نعرفه</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>521</td>\n",
       "      <td>يا عم ده جلده ومش بيطلع جنيه</td>\n",
       "      <td>1</td>\n",
       "      <td>الشخص ده بخيل جدا</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>450</td>\n",
       "      <td>فلورنتينو بيريز رايح يعطي كأس شامبينز ليڨ للخل...</td>\n",
       "      <td>1</td>\n",
       "      <td>فلورنتينو بيريز باغي يشري مبامبي من البياسجي ب...</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>65</td>\n",
       "      <td>ياخبر النهارده بفلوس بكره يبقى ببلاش</td>\n",
       "      <td>1</td>\n",
       "      <td>اى ان ما استعجلت عليه وتريد دفع فيه المال بالص...</td>\n",
       "      <td>nile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>23</td>\n",
       "      <td>بوتفليقة قالكم رايحين تعيشو كي الملوك</td>\n",
       "      <td>0</td>\n",
       "      <td>بوتفليقة قالكم رايحين تعيشو في جنة</td>\n",
       "      <td>magreb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id                                              tweet  sarcastic  \\\n",
       "0           6  الدنيا عاملة زى اوضة وصالة كل ما امشى فى حته ا...          1   \n",
       "1         218                        لا وانت الشهادة لله وفي اوي          1   \n",
       "2          92      من قير كوكا لا تحدثني عل قاز هذا راه بيت شعري          1   \n",
       "3          76  السعودية : لا أعرفه ، لذا لن أضيفه .المصرية :ا...          1   \n",
       "4         244  ساصدق قزانة الدشرة و مانامنش اتصالات الجزائر ا...          1   \n",
       "..        ...                                                ...        ...   \n",
       "293         6  الدنيا عاملة زى اوضة وصالة كل ما امشى فى حته ا...          1   \n",
       "294       521                       يا عم ده جلده ومش بيطلع جنيه          1   \n",
       "295       450  فلورنتينو بيريز رايح يعطي كأس شامبينز ليڨ للخل...          1   \n",
       "296        65               ياخبر النهارده بفلوس بكره يبقى ببلاش          1   \n",
       "297        23              بوتفليقة قالكم رايحين تعيشو كي الملوك          0   \n",
       "\n",
       "                                              rephrase dialect  \n",
       "0              الدنيا صغيرة ودايما نلاقى فيها حد نعرفه    nile  \n",
       "1                                             انت خائن    nile  \n",
       "2                                  كوكاكولا أفضل مشروب  magreb  \n",
       "3       ان المصريين بيضيفوا ناس كتير على مواقع التواصل    nile  \n",
       "4                               أتصالات الجزائر كادبين  magreb  \n",
       "..                                                 ...     ...  \n",
       "293            الدنيا صغيرة ودايما نلاقى فيها حد نعرفه    nile  \n",
       "294                                  الشخص ده بخيل جدا    nile  \n",
       "295  فلورنتينو بيريز باغي يشري مبامبي من البياسجي ب...  magreb  \n",
       "296  اى ان ما استعجلت عليه وتريد دفع فيه المال بالص...    nile  \n",
       "297                 بوتفليقة قالكم رايحين تعيشو في جنة  magreb  \n",
       "\n",
       "[298 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>يا زبدة لا تسيحي/ تذوبي</td>\n",
       "      <td>0</td>\n",
       "      <td>يا دهينة لا تنكتين</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ياسمين صبرى مش كويسة فى التمثيل</td>\n",
       "      <td>0</td>\n",
       "      <td>مين اللي أقنع ياسمين صبري أنها تمثل</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ابو بلاش كتر منه كتير</td>\n",
       "      <td>1</td>\n",
       "      <td>الشئ المجانى خذ منه كتير لانه غنيمه</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>انت متكبر لكن لا تملك ما يدفعك للتكبر</td>\n",
       "      <td>0</td>\n",
       "      <td>تحسب نفسك حاجة و انت متسواش نص دجاجة</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>معنديش فكه تاخد برميل بترول</td>\n",
       "      <td>1</td>\n",
       "      <td>سعر البترول يصل لادني مستوي</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>الراجل اللى مش معاه فلوس ملوش لازمة</td>\n",
       "      <td>0</td>\n",
       "      <td>شنب ماتحته فلوس يحتاج موس</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>قبل الجواز اغسل وش مراتك</td>\n",
       "      <td>1</td>\n",
       "      <td>لاز تشوف مراتك قبل الجواز بدون ميكب</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>شخص ليس له اي جدوي</td>\n",
       "      <td>0</td>\n",
       "      <td>يالك مش شخص عظيم جدا</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>معنديش مانع اخدك علي قد عقلك بس الاقيه الاول</td>\n",
       "      <td>1</td>\n",
       "      <td>معنديش مانع اتكلم معاك بس انت معندكش فكر اصلا</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>قلبي على ولدى انفطر ، وقلب ولدى على حجر</td>\n",
       "      <td>1</td>\n",
       "      <td>شخص لا يكترث بأهله الى يحبونه بالرغم انهم يراعونه</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id                                         tweet  sarcastic  \\\n",
       "0         NaN                       يا زبدة لا تسيحي/ تذوبي          0   \n",
       "1         NaN               ياسمين صبرى مش كويسة فى التمثيل          0   \n",
       "2         NaN                         ابو بلاش كتر منه كتير          1   \n",
       "3         NaN         انت متكبر لكن لا تملك ما يدفعك للتكبر          0   \n",
       "4         NaN                   معنديش فكه تاخد برميل بترول          1   \n",
       "..        ...                                           ...        ...   \n",
       "195       NaN           الراجل اللى مش معاه فلوس ملوش لازمة          0   \n",
       "196       NaN                      قبل الجواز اغسل وش مراتك          1   \n",
       "197       NaN                            شخص ليس له اي جدوي          0   \n",
       "198       NaN  معنديش مانع اخدك علي قد عقلك بس الاقيه الاول          1   \n",
       "199       NaN       قلبي على ولدى انفطر ، وقلب ولدى على حجر          1   \n",
       "\n",
       "                                              rephrase  dialect  \n",
       "0                                   يا دهينة لا تنكتين      NaN  \n",
       "1                  مين اللي أقنع ياسمين صبري أنها تمثل      NaN  \n",
       "2                  الشئ المجانى خذ منه كتير لانه غنيمه      NaN  \n",
       "3                 تحسب نفسك حاجة و انت متسواش نص دجاجة      NaN  \n",
       "4                          سعر البترول يصل لادني مستوي      NaN  \n",
       "..                                                 ...      ...  \n",
       "195                          شنب ماتحته فلوس يحتاج موس      NaN  \n",
       "196                لاز تشوف مراتك قبل الجواز بدون ميكب      NaN  \n",
       "197                               يالك مش شخص عظيم جدا      NaN  \n",
       "198      معنديش مانع اتكلم معاك بس انت معندكش فكر اصلا      NaN  \n",
       "199  شخص لا يكترث بأهله الى يحبونه بالرغم انهم يراعونه      NaN  \n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id                                         tweet  sarcastic  \\\n",
      "0         NaN                       يا زبدة لا تسيحي/ تذوبي          0   \n",
      "1         NaN               ياسمين صبرى مش كويسة فى التمثيل          0   \n",
      "2         NaN                         ابو بلاش كتر منه كتير          1   \n",
      "3         NaN         انت متكبر لكن لا تملك ما يدفعك للتكبر          0   \n",
      "4         NaN                   معنديش فكه تاخد برميل بترول          1   \n",
      "..        ...                                           ...        ...   \n",
      "195       NaN           الراجل اللى مش معاه فلوس ملوش لازمة          0   \n",
      "196       NaN                      قبل الجواز اغسل وش مراتك          1   \n",
      "197       NaN                            شخص ليس له اي جدوي          0   \n",
      "198       NaN  معنديش مانع اخدك علي قد عقلك بس الاقيه الاول          1   \n",
      "199       NaN       قلبي على ولدى انفطر ، وقلب ولدى على حجر          1   \n",
      "\n",
      "                                              rephrase  dialect  \n",
      "0                                   يا دهينة لا تنكتين      NaN  \n",
      "1                  مين اللي أقنع ياسمين صبري أنها تمثل      NaN  \n",
      "2                  الشئ المجانى خذ منه كتير لانه غنيمه      NaN  \n",
      "3                 تحسب نفسك حاجة و انت متسواش نص دجاجة      NaN  \n",
      "4                          سعر البترول يصل لادني مستوي      NaN  \n",
      "..                                                 ...      ...  \n",
      "195                          شنب ماتحته فلوس يحتاج موس      NaN  \n",
      "196                لاز تشوف مراتك قبل الجواز بدون ميكب      NaN  \n",
      "197                               يالك مش شخص عظيم جدا      NaN  \n",
      "198      معنديش مانع اتكلم معاك بس انت معندكش فكر اصلا      NaN  \n",
      "199  شخص لا يكترث بأهله الى يحبونه بالرغم انهم يراعونه      NaN  \n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "y3tY3ECJDPaz",
    "outputId": "b4ff4686-f568-4f3c-8eef-006485c6d660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test total 200 sarcastic 100 non sarcastic 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'not_sarcastic'), Text(1, 0, 'sarcastic')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_test.sarcastic)\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_test.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_test.sarcastic)\n",
    "print(\"test total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "\n",
    "ax.set_xticklabels(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val total 298 sarcastic 186 non sarcastic 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_val.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names);\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_val.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_val.sarcastic)\n",
    "print(\"Val total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train total 1521 sarcastic 759 non sarcastic 762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAELCAYAAAA2mZrgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYQUlEQVR4nO3dfbRddX3n8ffHoDyqEA2ZSJgSXREmaEWNqQ/LR7TgVA2j0sbRNlpqHBd11GmtoV3TOm3ToUuc0VFpV3yMthVTUUldPoCZou34gAFRCBhJiUJMSq6KIlWjxO/8sX/ZHG5uwg1k33tJ3q+1ztp7/87e+3xPcs/5nP3026kqJEkCuN90FyBJmjkMBUlSz1CQJPUMBUlSz1CQJPUMBUlSb9BQSPL6JBuTXJvkQ0mOSDI7yWVJbmjD40bmPy/J5iSbkpwxZG2SpD1lqOsUkpwA/DOwqKp+kmQt8ElgEfD9qjo/yUrguKp6Y5JFwIeAJcDDgM8Cj6yqXYMUKEnaw9C7jw4DjkxyGHAUsA1YCqxpz68BzmrjS4GLqmpnVW0BNtMFhCRpihw21Iqr6jtJLgBuAn4CXFpVlyaZW1Xb2zzbkxzfFjkB+NLIKra2trtIsgJYAXD00Uc//pRTThnqLUjSQenKK6/8blXNmei5wUKhHStYCiwAfgD8fZKX7WuRCdr22LdVVauB1QCLFy+uDRs23PtiJekQkuTbe3tuyN1Hzwa2VNVYVf0c+CjwZOCWJPNaYfOAHW3+rcCJI8vPp9vdJEmaIkOGwk3AE5MclSTA6cD1wDpgeZtnOXBJG18HLEtyeJIFwELgigHrkySNM+QxhS8n+QhwFXAH8FW63T7HAGuTnEMXHGe3+Te2M5Sua/Of65lHkjS1BjsldSp4TEGS9l+SK6tq8UTPeUWzJKlnKEiSeoaCJKlnKEiSeoaCJKk32Cmp9xWPf8MHprsEzUBXvvm3prsEaVoc8qEgzVQ3/emjp7sEzUD//o+vGXT97j6SJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUGC4UkJye5euRxW5LXJZmd5LIkN7ThcSPLnJdkc5JNSc4YqjZJ0sQGC4Wq2lRVp1XVacDjgR8DHwNWAuuraiGwvk2TZBGwDDgVOBO4MMmsoeqTJO1pqnYfnQ78S1V9G1gKrGnta4Cz2vhS4KKq2llVW4DNwJIpqk+SxNSFwjLgQ218blVtB2jD41v7CcDNI8tsbW2SpCkyeCgkeQDwAuDv727WCdpqgvWtSLIhyYaxsbEDUaIkqZmKLYXnAldV1S1t+pYk8wDacEdr3wqcOLLcfGDb+JVV1eqqWlxVi+fMmTNg2ZJ06JmKUHgJd+46AlgHLG/jy4FLRtqXJTk8yQJgIXDFFNQnSWoGvR1nkqOA5wCvGmk+H1ib5BzgJuBsgKramGQtcB1wB3BuVe0asj5J0l0NGgpV9WPgIePavkd3NtJE868CVg1ZkyRp77yiWZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUGzQUkhyb5CNJvpHk+iRPSjI7yWVJbmjD40bmPy/J5iSbkpwxZG2SpD0NvaXwNuDTVXUK8BjgemAlsL6qFgLr2zRJFgHLgFOBM4ELk8wauD5J0ojBQiHJg4CnAe8BqKqfVdUPgKXAmjbbGuCsNr4UuKiqdlbVFmAzsGSo+iRJexpyS+HhwBjwviRfTfLuJEcDc6tqO0AbHt/mPwG4eWT5ra3tLpKsSLIhyYaxsbEBy5ekQ8+QoXAY8Djgr6rqscC/0XYV7UUmaKs9GqpWV9Xiqlo8Z86cA1OpJAkYNhS2Alur6stt+iN0IXFLknkAbbhjZP4TR5afD2wbsD5J0jiDhUJV/Stwc5KTW9PpwHXAOmB5a1sOXNLG1wHLkhyeZAGwELhiqPokSXs6bOD1vwb42yQPAG4EXkEXRGuTnAPcBJwNUFUbk6ylC447gHOratfA9UmSRgwaClV1NbB4gqdO38v8q4BVQ9YkSdo7r2iWJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSz1CQJPUMBUlSb9BQSPKtJNckuTrJhtY2O8llSW5ow+NG5j8vyeYkm5KcMWRtkqQ9TcWWwjOr6rSq2n2v5pXA+qpaCKxv0yRZBCwDTgXOBC5MMmsK6pMkNdOx+2gpsKaNrwHOGmm/qKp2VtUWYDOwZOrLk6RD19ChUMClSa5MsqK1za2q7QBteHxrPwG4eWTZra3tLpKsSLIhyYaxsbEBS5ekQ89hA6//KVW1LcnxwGVJvrGPeTNBW+3RULUaWA2wePHiPZ6XJN1zg24pVNW2NtwBfIxud9AtSeYBtOGONvtW4MSRxecD24asT5J0V4OFQpKjkzxw9zjwq8C1wDpgeZttOXBJG18HLEtyeJIFwELgiqHqkyTtacjdR3OBjyXZ/Tp/V1WfTvIVYG2Sc4CbgLMBqmpjkrXAdcAdwLlVtWvA+iRJ4wwWClV1I/CYCdq/B5y+l2VWAauGqkmStG9e0SxJ6hkKkqSeoSBJ6hkKkqSeoSBJ6hkKkqSeoSBJ6hkKkqSeoSBJ6hkKkqTepEIhyfrJtEmS7tv22fdRkiOAo4CHtnsp777nwYOAhw1cmyRpit1dh3ivAl5HFwBXcmco3Aa8c7iyJEnTYZ+hUFVvA96W5DVV9fYpqkmSNE0m1XV2Vb09yZOBk0aXqaoPDFSXJGkaTCoUknwQeARwNbD7xjcFGAqSdBCZ7E12FgOLqqqGLEaSNL0me53CtcC/G7IQSdL0m2woPBS4Lslnkqzb/ZjMgklmJflqkk+06dlJLktyQxseNzLveUk2J9mU5Iz9fzuSpHtjsruP3nQvXuO1wPV01zYArATWV9X5SVa26TcmWQQsA06lOwX2s0keWVW7JlqpJOnAm+zZR5+7JytPMh/4NWAV8N9a81LgGW18DXA58MbWflFV7QS2JNkMLAG+eE9eW5K0/ybbzcWPktzWHj9NsivJbZNY9K3AHwC/GGmbW1XbAdrw+NZ+AnDzyHxbW9v4WlYk2ZBkw9jY2GTKlyRN0qRCoaoeWFUPao8jgBcB79jXMkmeB+yoqisnWUsmaNvjbKeqWl1Vi6tq8Zw5cya5aknSZEz2mMJdVNXH2/GAfXkK8IIk/xE4AnhQkr8Bbkkyr6q2J5kH7GjzbwVOHFl+PrDtntQnSbpnJrv76IUjjxcnOZ8JfsWPqqrzqmp+VZ1EdwD5/1bVy4B1wPI223Lgkja+DliW5PAkC4CFwBX7/5YkSffUZLcUnj8yfgfwLboDw/fE+cDaJOcANwFnA1TVxiRrgevaa5zrmUeSNLUme/bRK+7Ni1TV5XRnGVFV3wNO38t8q+jOVJIkTYPJ7j6an+RjSXYkuSXJxe10U0nSQWSyVzS/j26f/8PoThP9h9YmSTqITDYU5lTV+6rqjvZ4P+D5oJJ0kJlsKHw3yctaP0azkrwM+N6QhUmSpt5kQ+G3gV8H/hXYDrwYuFcHnyVJM89kT0n9M2B5Vd0KXU+nwAV0YSFJOkhMdkvhl3cHAkBVfR947DAlSZKmy2RD4X7j7nswm3vYRYYkaeaa7Bf7W4AvJPkIXfcWv44XmUnSQWeyVzR/IMkG4Fl0vZm+sKquG7QySdKUm/QuoBYCBoEkHcQme0xBknQIMBQkST1DQZLUMxQkST1DQZLUMxQkST1DQZLUGywUkhyR5IokX0uyMcn/aO2zk1yW5IY2HO0+47wkm5NsSnLGULVJkiY25JbCTuBZVfUY4DTgzCRPBFYC66tqIbC+TZNkEbAMOBU4E7gwyawB65MkjTNYKFTn9jZ5//YoYCmwprWvAc5q40uBi6pqZ1VtATYDS4aqT5K0p0GPKbS7tF0N7AAuq6ovA3OrajtAGx7fZj8BuHlk8a2tbfw6VyTZkGTD2NjYkOVL0iFn0FCoql1VdRowH1iS5FH7mD0TrWKCda6uqsVVtXjOHG8TLUkH0pScfVRVPwAupztWcEuSeQBtuKPNthU4cWSx+cC2qahPktQZ8uyjOUmObeNHAs8GvgGsA5a32ZYDl7TxdcCyJIcnWQAsBK4Yqj5J0p6GvHvaPGBNO4PofsDaqvpEki8Ca5OcA9wEnA1QVRuTrKXrnvsO4Nyq2jVgfZKkcQYLhar6OhPcx7mqvgecvpdlVuEd3SRp2nhFsySpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqDhUKSE5P8Y5Lrk2xM8trWPjvJZUluaMPjRpY5L8nmJJuSnDFUbZKkiQ25pXAH8HtV9R+AJwLnJlkErATWV9VCYH2bpj23DDgVOBO4MMmsAeuTJI0zWChU1faquqqN/wi4HjgBWAqsabOtAc5q40uBi6pqZ1VtATYDS4aqT5K0pyk5ppDkJOCxwJeBuVW1HbrgAI5vs50A3Dyy2NbWJkmaIoOHQpJjgIuB11XVbfuadYK2mmB9K5JsSLJhbGzsQJUpSWLgUEhyf7pA+Nuq+mhrviXJvPb8PGBHa98KnDiy+Hxg2/h1VtXqqlpcVYvnzJkzXPGSdAga8uyjAO8Brq+q/zXy1DpgeRtfDlwy0r4syeFJFgALgSuGqk+StKfDBlz3U4DfBK5JcnVr+0PgfGBtknOAm4CzAapqY5K1wHV0Zy6dW1W7BqxPkjTOYKFQVf/MxMcJAE7fyzKrgFVD1SRJ2jevaJYk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVLPUJAk9QwFSVJvsFBI8t4kO5JcO9I2O8llSW5ow+NGnjsvyeYkm5KcMVRdkqS9G3JL4f3AmePaVgLrq2ohsL5Nk2QRsAw4tS1zYZJZA9YmSZrAYKFQVZ8Hvj+ueSmwpo2vAc4aab+oqnZW1RZgM7BkqNokSROb6mMKc6tqO0AbHt/aTwBuHplva2uTJE2hmXKgORO01YQzJiuSbEiyYWxsbOCyJOnQMtWhcEuSeQBtuKO1bwVOHJlvPrBtohVU1eqqWlxVi+fMmTNosZJ0qJnqUFgHLG/jy4FLRtqXJTk8yQJgIXDFFNcmSYe8w4ZacZIPAc8AHppkK/AnwPnA2iTnADcBZwNU1cYka4HrgDuAc6tq11C1SZImNlgoVNVL9vLU6XuZfxWwaqh6JEl3b6YcaJYkzQCGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSpZyhIknozLhSSnJlkU5LNSVZOdz2SdCiZUaGQZBbwTuC5wCLgJUkWTW9VknTomFGhACwBNlfVjVX1M+AiYOk01yRJh4zDpruAcU4Abh6Z3gr8yugMSVYAK9rk7Uk2TVFth4KHAt+d7iJmglywfLpL0F35t7nbn+RArOWX9vbETAuFid5t3WWiajWwemrKObQk2VBVi6e7Dmk8/zanzkzbfbQVOHFkej6wbZpqkaRDzkwLha8AC5MsSPIAYBmwbpprkqRDxozafVRVdyT5XeAzwCzgvVW1cZrLOpS4W04zlX+bUyRVdfdzSZIOCTNt95EkaRoZCpKknqEg6aCS5A/HTX9humq5LzIU7sOSvDzJw6a7jn0ZX2OSd9t1iSaS5ECd+HKXUKiqJx+g9R4SZtTZR9pvLweuZYBrOZIcVlV3HIBVvZyRGqvqdw7AOjWDJTkaWEt3ndEs4M+Ak4HnA0cCXwBeVVWV5PI2/RRgXZLPA28DjgZ2AqcDDwE+2NoAfreqvpBkHvBh4EF032WvBn4NODLJ1cDGqnppktur6phW2x8Avwn8AvhUVdnp5nhV5WOGPICTgOuBdwEbgUvpPkSnAV8Cvg58DDgOeDFwO7AJuBo4ci/rPB+4ri17QWt7PvBl4KvAZ4G5rf1NdKf+XQr8HTC3vd7X2uPJbb6PA1e2Gle0tlnA++kC4Brg9RPVCFwOLG7LnAlc1da9frr//X0csL/jFwHvGpl+MDB7ZPqDwPPb+OXAhW38AcCNwBPa9O4v+6OAI1rbQmBDG/894I9G/v4e2MZvH1fP7W34XLoAOqpNz7637/VgfEx7AT5G/jO6ULgDOK1NrwVe1r7Qn97a/hR4axvvv2D3sr7Z7Qt596nHx7bhcSNtvwO8pY2/qX3ZH9mmPwy8ro3PAh68e71teGQLgYcAjwcuG3ntYyeqcfc0MIeun6sFo+v0cd9/AI8EtgB/CTy1tb2I7ofINcB3gJUjfw9Pb+OPBv7fBOt7cAuSa+h+XPy4tT8N2Nz+bk8bmX9vofAW4JXT/e8z0x8eU5h5tlTV1W38SuARdF+wn2tta+g+DJNxG/BT4N1JXgj8uLXPBz6T5BrgDcCpI8usq6qftPFnAX8FUFW7quqHrf2/Jvka3dbLiXS/3m4EHp7k7UnObK+9L08EPl9VW9r6vz/J96QZrqq+Sfcj4Rrgfyb5Y+BC4MVV9Wi6LeEjRhb5tzYM4/o6a14P3AI8hu4HxQPa63ye7rPwHeCDSX7rbkrb2/o1wlCYeXaOjO8Cjr2nK6rumMAS4GLgLODT7am3A+9oH9BXMfEHdEJJngE8G3hSVT2GbhfUEVV1K92H9nLgXODdd1OeH9CDVDux4MdV9TfABcDj2lPfTXIM3W7FiXwDeFiSJ7T1PLAdfH4wsL2qfkF3PGBWe/6XgB1V9S7gPSOv8/Mk959g/ZcCv53kqLb87Hv5Vg9KHmie+X4I3JrkqVX1T3Qfit1bDT8CHri3BdsH8Kiq+mSSL9FtakP3IftOG99XH9Hr6Q7evbXdAOnotuytVfXjJKfQ/eInyUOBn1XVxUn+he74wr5q/CLwziQLqmpLktluLRw0Hg28OckvgJ/T/Q2dRbfl8C26Ps72UFU/S/IbwNuTHAn8hO4HyIXAxUnOBv6RO3+4PAN4Q5Kf0x272r2lsBr4epKrquqlI+v/dJLTgA1JfgZ8knFnKsluLmaUJCcBn6iqR7Xp3weOoTuw+9d0B9xuBF5RVbcmeRHwF3QfnieN7PbZvb55wCV0WwKhO9C8JslS4H/TBcOX6A7sPSPJm+j2v17Qlp9L9wF7ON1Wy6vpDgx/nO7eF5vojg28CbgVeB93bn2eV1WfGl8j8Cng96tqQ5LntufuR/eL7zn3/l9R0r1hKEiSeh5TkCT1PKZwkEjyMWDBuOY3VtVnpqMeSfdN7j6SJPXcfSRJ6hkKkqSeoaCDRpI/SrIxydeTXJ3kV6a7pruT5KQk1+7H/O9PsreLv+71+iUPNOugkORJwPOAx1XVznYx3QP2Y/kD1SusdJ/mloIOFvOA71bVToCq+m5VbQNI8sdJvpLk2iSrk6S1X57kL5J8Dnhtkick+UKSryW5onWzcFKSf0pyVXs8uS07L8nn2xbJtUme2tpvT/KXSa5M8tkkS9rr3JjkBZN9M0le2Wr+WpKLd3fN0Dy71fTNJM9r889K8ua2zNeTvOqA/KvqkGMo6GBxKXBi+6K8MMnTR557R1U9oV0pfiTdFsVux1bV0+n6g/ow8NrWp9Oz6a7C3gE8p6oeB/wG8H/acv8Z+ExVnUbX59PVrf1o4PKqejxdFx9/DjwH+E90PdxO1kdbzY+h6079nJHnTgKeTnfvgL9OckR7/odV9QTgCcArk4w/RVm6W+4+0kGhqm5P8njgqcAzgQ8nWVlV7wee2W6uchRdd+IbgX9oi364DU+m63TtK219t0F/w5h3tD5zdtF1Cw1d/z3vbR2vfXykZ9ufcWfHg9cAO6vq561H2pP24y09Ksmf03WIeAwwer3J2tY53A1JbgROAX4V+OWR4w0Ppuu99pv78ZqSoaCDR1Xtouul9fL2Jbw8yUV0HaotrqqbW/9O97Tb5vvRdUVOVX0+ydPofq1/MMmbq+oDwM/rzot/fkHr9baqfpH9u93k+4GzquprSV5O1/lb/1bHv/VW/2vGX6zY+tOSJs3dRzooJDk5ycKRptOAb3NnAExVt80HygOB7W1L5KXjnjs7yf2SPIKus8JNdFsSr97dZXSSR7atHGm/uKWgg8UxdF0uH0t397rNdLcK/UGSdzF13TbfEycn2Toy/Xrgv9PdqezbrfbR7sc30XWfPhf4L1X10yTvpts9dVU7kD5G1121tF/s5kKS1HP3kSSpZyhIknqGgiSpZyhIknqGgiSpZyhIknqGgiSp9/8BOV5A33baIC0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df_train.sarcastic)\n",
    "plt.xlabel('Sarcasm Label')\n",
    "ax.set_xticklabels(class_names)\n",
    "s_t = 0\n",
    "ns_t =0\n",
    "for i in df_train.sarcastic:\n",
    "    if i == 1:\n",
    "        s_t+=1\n",
    "    else:\n",
    "        ns_t+=1\n",
    "l_count = len(df_train.sarcastic)\n",
    "print(\"train total\", l_count, \"sarcastic\", s_t, \"non sarcastic\", ns_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'CAMeL-Lab/bert-base-arabic-camelbert-mix'\n",
    "bertweet = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMSr7C-F_sey"
   },
   "source": [
    "> You can use a cased and uncased version of BERT and tokenizer. I've experimented with both. The cased version works better. Intuitively, that makes sense, since \"BAD\" might convey more sarcasm than \"bad\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPTweetDataset(Dataset):\n",
    "\n",
    "  def __init__(self, tweets, targets, tokenizer, max_len):\n",
    "    self.tweets = tweets\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    tweet = str(self.tweets[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      tweet,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "      'tweet_text': tweet,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xz3ZOQXVPCwh",
    "outputId": "dd8d2844-3b22-425d-dc40-725f7f46e52a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521, 5), (298, 5), (200, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "We also need to create a couple of data loaders. Here's a helper function to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  tweets=[]\n",
    "  for index, row in df.iterrows():\n",
    "    text=str(row['tweet'])+tokenizer.sep_token+str(row['rephrase'])\n",
    "    tweets.append(text)\n",
    "    \n",
    "  ds = GPTweetDataset(\n",
    "    tweets=tweets,\n",
    "    targets=df.sarcastic.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vODDxMKsPHqI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Y93ldSN47FeT",
    "outputId": "ee6eaa1a-3f03-4e18-c059-02dbf8b8bc14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['tweet_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "IdU4YVqb7N8M",
    "outputId": "1f67fe37-6634-484f-caa2-1517e80a29d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n",
      "torch.Size([16, 150])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SarcasmClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SarcasmClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask, return_dict=False)\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SarcasmClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mz7p__CqdaMO",
    "outputId": "7a933577-8c04-42f3-c3ea-ecb9c1c30a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n",
      "torch.Size([16, 150])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "2rTCj46Zamry",
    "outputId": "04ecb643-ccda-461f-886f-aefe01f9a248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150])\n",
      "torch.Size([16, 150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3028, 0.6972],\n",
       "        [0.3035, 0.6965],\n",
       "        [0.3300, 0.6700],\n",
       "        [0.5875, 0.4125],\n",
       "        [0.4583, 0.5417],\n",
       "        [0.2805, 0.7195],\n",
       "        [0.6214, 0.3786],\n",
       "        [0.4565, 0.5435],\n",
       "        [0.3520, 0.6480],\n",
       "        [0.5057, 0.4943],\n",
       "        [0.4918, 0.5082],\n",
       "        [0.4593, 0.5407],\n",
       "        [0.5094, 0.4906],\n",
       "        [0.5085, 0.4915],\n",
       "        [0.3489, 0.6511],\n",
       "        [0.3927, 0.6073]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)\n",
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "#weights = [float(s_t)/(s_t+ns_t),float(2*ns_t)/(s_t+ns_t)] #as class distribution\n",
    "#class_weights = torch.FloatTensor(weights).cuda()\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device) #CrossEntropyLoss() has already included a softmax layer inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    \n",
    "    outputs=[]\n",
    "    try:\n",
    "      outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask)\n",
    "    except:\n",
    "      print(\"An exception occurred\")\n",
    "      print(\"input_ids=\",input_ids.shape, \"attention_mask=\" , attention_mask.shape)\n",
    "      continue\n",
    "    \n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs=[]\n",
    "      try:\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "      except:\n",
    "        print(\"An exception occurred\")\n",
    "        print(\"input_ids=\",input_ids.shape, \"attention_mask=\" , attention_mask.shape)\n",
    "        continue\n",
    "    \n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "1zhHoFNsxufs",
    "outputId": "2f11710a-700e-4933-b57e-5d50e5ed1f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4550803956450788 accuracy 0.7712031558185405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5289297713653037 accuracy 0.8758389261744967\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.04315572747183675 accuracy 0.9875082182774491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.4467540739673208 accuracy 0.9429530201342282\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.001279456351880981 accuracy 0.9993425378040763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6117826016343315 accuracy 0.9228187919463087\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0061469252174977855 accuracy 0.9993425378040763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5098584998718012 accuracy 0.9228187919463087\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 8.871035030703449e-05 accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5141519522522254 accuracy 0.9261744966442953\n",
      "\n",
      "CPU times: user 55.7 s, sys: 5.36 s, total: 1min 1s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "      )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "      )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        best_accuracy = val_acc\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3HZb3NWFtFf"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jS3gJ_qBEljD",
    "outputId": "21f968b6-fd29-4e74-dee0-8dc9eacd301e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  tweet_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"tweet_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      tweet_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return tweet_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  val_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.9327    0.8661    0.8981       112\n",
      "    sarcastic     0.9227    0.9624    0.9421       186\n",
      "\n",
      "     accuracy                         0.9262       298\n",
      "    macro avg     0.9277    0.9142    0.9201       298\n",
      " weighted avg     0.9264    0.9262    0.9256       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "zHdPZr60-0c_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/sknigam/anaconda3/envs/tweeter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_tweet_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "L8a9_8-ND3Is",
    "outputId": "9b2c48cc-b62e-41f3-dba5-af90457a37de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "not_sarcastic     0.8936    0.8400    0.8660       100\n",
      "    sarcastic     0.8491    0.9000    0.8738       100\n",
      "\n",
      "     accuracy                         0.8700       200\n",
      "    macro avg     0.8713    0.8700    0.8699       200\n",
      " weighted avg     0.8713    0.8700    0.8699       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08.sentiment-analysis-with-bert.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "eea26f656b4850182355e5ba3607fa37b1d1e122add4b8b2e4fad1e2abcd3873"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
